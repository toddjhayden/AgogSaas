# AgogSaaS Alerting Rules
# Purpose: Define critical alerts for operations team
# All alerts include actionable guidance

groups:
  # ==========================================================================
  # CRITICAL ALERTS - Immediate Action Required
  # ==========================================================================

  - name: agogsaas_critical
    interval: 30s
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          team: operations
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."
          runbook: "https://docs.agogsaas.com/runbooks/service-down"
          actions: |
            1. Check service logs: docker logs {{ $labels.job }}
            2. Verify service health: curl http://{{ $labels.instance }}/health
            3. Restart service if needed: docker-compose restart {{ $labels.job }}
            4. Check resource usage: docker stats {{ $labels.job }}
            5. Escalate to on-call engineer if issue persists

      # Edge Computer Offline
      - alert: EdgeFacilityOffline
        expr: up{tier="edge"} == 0
        for: 5m
        labels:
          severity: critical
          team: customer-success
        annotations:
          summary: "Edge facility {{ $labels.facility_name }} is offline"
          description: "{{ $labels.facility_name }} ({{ $labels.facility_id }}) has been offline for more than 5 minutes."
          runbook: "https://docs.agogsaas.com/runbooks/edge-offline"
          actions: |
            1. Contact facility IT: Check facility_contacts table for phone/email
            2. Verify edge computer is powered on
            3. Check internet connectivity at facility
            4. Check VPN tunnel status: tailscale status
            5. Review edge logs if accessible
            6. Offer cloud fallback mode if facility destroyed
            7. Escalate to customer success manager

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (job, instance)
            /
            sum(rate(http_requests_total[5m])) by (job, instance)
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          team: engineering
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} on {{ $labels.instance }} has error rate > 5% for the last 5 minutes."
          runbook: "https://docs.agogsaas.com/runbooks/high-error-rate"
          actions: |
            1. Check application logs for stack traces
            2. Review recent deployments (possible bad release?)
            3. Check database connectivity and performance
            4. Check external API dependencies (Stripe, SendGrid, etc)
            5. Consider rollback if error rate continues to rise
            6. Monitor user impact in Grafana dashboard

      # Database Connection Pool Exhausted
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connection_pool_size >= db_connection_pool_max * 0.9
        for: 3m
        labels:
          severity: critical
          team: engineering
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $labels.job }} is using {{ $value }}% of available database connections."
          runbook: "https://docs.agogsaas.com/runbooks/db-connection-pool"
          actions: |
            1. Check for connection leaks in application code
            2. Identify slow queries: SELECT * FROM pg_stat_activity WHERE state = 'active'
            3. Consider increasing max_connections (current: 200)
            4. Review connection timeout settings
            5. Check if microservices are properly closing connections

      # Replication Lag High
      - alert: DatabaseReplicationLagHigh
        expr: pg_replication_lag_seconds > 60
        for: 2m
        labels:
          severity: warning
          team: engineering
        annotations:
          summary: "Database replication lag is high"
          description: "Replication lag is {{ $value }}s on {{ $labels.instance }}. Data sync delayed."
          runbook: "https://docs.agogsaas.com/runbooks/replication-lag"
          actions: |
            1. Check network connectivity between regions
            2. Identify large transactions blocking replication
            3. Check replication slot status: SELECT * FROM pg_replication_slots
            4. Monitor WAL generation rate
            5. Consider temporarily pausing non-critical writes

  # ==========================================================================
  # WARNING ALERTS - Action Required Soon
  # ==========================================================================

  - name: agogsaas_warnings
    interval: 60s
    rules:
      # High Response Time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 10m
        labels:
          severity: warning
          team: engineering
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "P95 response time is {{ $value }}s on {{ $labels.job }}. Users experiencing slowness."
          runbook: "https://docs.agogsaas.com/runbooks/high-response-time"
          actions: |
            1. Check slow query log in PostgreSQL
            2. Review APM traces in Grafana
            3. Check Redis cache hit rate
            4. Profile application CPU usage
            5. Check for N+1 queries in GraphQL resolvers
            6. Consider adding database indexes

      # Disk Space Low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.15
        for: 5m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: "Disk space low on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanizePercentage }} full on {{ $labels.instance }}."
          runbook: "https://docs.agogsaas.com/runbooks/disk-space-low"
          actions: |
            1. Check disk usage: df -h
            2. Find large files: du -sh /* | sort -rh | head -10
            3. Clean up Docker images: docker system prune -a
            4. Rotate logs: journalctl --vacuum-time=7d
            5. Expand volume if needed
            6. Review data retention policies

      # Memory Usage High
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.90
        for: 5m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}."
          runbook: "https://docs.agogsaas.com/runbooks/high-memory"
          actions: |
            1. Check memory usage by process: docker stats
            2. Review application memory leaks
            3. Check Redis memory usage and eviction policy
            4. Consider restarting memory-hungry services
            5. Scale horizontally if sustained high usage

      # CPU Usage High
      - alert: HighCPUUsage
        expr: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))) > 0.85
        for: 10m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}."
          runbook: "https://docs.agogsaas.com/runbooks/high-cpu"
          actions: |
            1. Identify CPU-intensive processes: top
            2. Check for infinite loops or runaway queries
            3. Review recent code changes
            4. Profile application CPU usage
            5. Scale horizontally if sustained high load

      # Redis Connection Errors
      - alert: RedisConnectionErrors
        expr: rate(redis_rejected_connections_total[5m]) > 0
        for: 3m
        labels:
          severity: warning
          team: engineering
        annotations:
          summary: "Redis connection errors detected"
          description: "Redis is rejecting connections on {{ $labels.instance }}."
          runbook: "https://docs.agogsaas.com/runbooks/redis-errors"
          actions: |
            1. Check Redis connection limit: redis-cli CONFIG GET maxclients
            2. Review application connection pooling
            3. Check Redis memory usage
            4. Look for connection leaks in application

      # NATS Connection Issues
      - alert: NATSConnectionIssues
        expr: nats_varz_slow_consumers > 0
        for: 5m
        labels:
          severity: warning
          team: engineering
        annotations:
          summary: "NATS has slow consumers"
          description: "NATS has {{ $value }} slow consumers. Message processing delayed."
          runbook: "https://docs.agogsaas.com/runbooks/nats-slow-consumers"
          actions: |
            1. Identify slow consumers: nats server report connections
            2. Check consumer processing logic
            3. Review message queue depth
            4. Consider adding consumer instances
            5. Check if consumers are blocking on I/O

  # ==========================================================================
  # BUSINESS METRICS ALERTS
  # ==========================================================================

  - name: agogsaas_business
    interval: 60s
    rules:
      # No Production Runs Active (during business hours)
      - alert: NoProductionRunsActive
        expr: active_production_runs == 0
        for: 30m
        labels:
          severity: warning
          team: customer-success
        annotations:
          summary: "No production runs active"
          description: "No production runs have been active for 30 minutes. Check if facilities are operating."
          actions: |
            1. Check facility status dashboard
            2. Verify it's during business hours (8am-6pm local time)
            3. Contact facility managers if unexpected
            4. Check if scheduled maintenance is occurring

      # Material Utilization Low
      - alert: LowMaterialUtilization
        expr: material_utilization_percentage < 75
        for: 1h
        labels:
          severity: info
          team: customer-success
        annotations:
          summary: "Material utilization below target"
          description: "Material utilization is {{ $value }}% (target: 85%). Potential waste detected."
          actions: |
            1. Review production planning efficiency
            2. Check nesting algorithm performance
            3. Identify facilities with lowest utilization
            4. Schedule customer training if needed

      # Failed Login Attempts Spike
      - alert: FailedLoginAttemptsSpike
        expr: rate(auth_failed_login_attempts_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "High rate of failed login attempts"
          description: "{{ $value }} failed login attempts per second. Possible brute force attack."
          runbook: "https://docs.agogsaas.com/runbooks/security-incident"
          actions: |
            1. Check login attempt logs for patterns
            2. Identify source IP addresses
            3. Enable rate limiting if not active
            4. Consider temporary IP blocks
            5. Notify security team immediately
            6. Check if legitimate users are having issues

  # ==========================================================================
  # SYNTHETIC MONITORING ALERTS
  # ==========================================================================

  - name: agogsaas_synthetic
    interval: 60s
    rules:
      # API Health Check Failed
      - alert: APIHealthCheckFailed
        expr: probe_success{job="blackbox-http"} == 0
        for: 3m
        labels:
          severity: critical
          team: operations
        annotations:
          summary: "API health check failed for {{ $labels.instance }}"
          description: "Health check endpoint {{ $labels.instance }} has been failing for 3 minutes."
          runbook: "https://docs.agogsaas.com/runbooks/health-check-failed"
          actions: |
            1. Check if service is running: docker ps
            2. Verify network connectivity
            3. Check service logs for errors
            4. Test endpoint manually: curl {{ $labels.instance }}
            5. Restart service if needed

      # Database Connection Check Failed
      - alert: DatabaseConnectionFailed
        expr: probe_success{job="blackbox-tcp", instance=~".*postgres.*"} == 0
        for: 2m
        labels:
          severity: critical
          team: operations
        annotations:
          summary: "Database connection check failed"
          description: "Cannot connect to {{ $labels.instance }} for 2 minutes."
          runbook: "https://docs.agogsaas.com/runbooks/db-connection-failed"
          actions: |
            1. Check if database is running: docker ps | grep postgres
            2. Check database logs: docker logs postgres-blue
            3. Verify network connectivity
            4. Check if max_connections exceeded
            5. Restart database if needed (coordinate with team first!)

  # ==========================================================================
  # BLUE-GREEN DEPLOYMENT ALERTS
  # ==========================================================================

  - name: agogsaas_deployment
    interval: 60s
    rules:
      # Environment Mismatch (both or neither active)
      - alert: BlueGreenEnvironmentMismatch
        expr: |
          (
            count(up{environment="blue", job="backend-blue"} == 1)
            +
            count(up{environment="green", job="backend-green"} == 1)
          ) != 1
        for: 5m
        labels:
          severity: warning
          team: operations
        annotations:
          summary: "Blue-green environment mismatch detected"
          description: "Either both blue and green are active, or neither is active. Check load balancer config."
          actions: |
            1. Check nginx active environment: cat /etc/nginx/conf.d/default.conf
            2. Verify intended active environment
            3. Check deployment logs
            4. Run switch script if needed: ./scripts/switch-blue-green.sh [blue|green]
